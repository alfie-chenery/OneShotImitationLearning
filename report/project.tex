\chapter{Technical Design}
\label{chap:technical}

\section{Project Scope}
By design this project aims to create a generalisable robot learning algorithm which will have applications in multiple problem scenarios. The generalisability comes from the ability to provide a robot with whatever demonstrations are relevant to solving the desired task, without requiring any changes to the core system. However, in the interest of keeping this project focused we choose to implement and analyse specifically an agent for generalised object manipulation tasks focused around grasping and moving simple objects. 
\\

\subsection{Environment specifics}
\label{subsec:pybullet-specifics}
For this project we conduct the experiments in a real time physics engine, called \socalled{PyBullet} \cite{pybullet}, available as an open source python package. As such the algorithms presented in this paper will be implemented in Python and the experimental results and evaluation will be conducted in this simulation space.\\
The specific robot arm used throughout this project is a simulation of the \socalled{Franka Panda robot arm}, which comes built in with PyBullet. This arm can be seen in \reffig{fig:arm-normal}.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/robot-arm.png}
        \caption{Normal view}
        \label{fig:arm-normal}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \includegraphics[width=\textwidth]{figures/robot-arm-wireframe.png}
        \caption{Wire-frame view}
        \label{fig:arm-wire}
    \end{subfigure}
    \caption{Franka panda robot arm in PyBullet simulation}
    \label{fig:robot-arm}
\end{figure}

This simulation and arm have a number of intricacies which posed a steep learning curve near the start of this project. This subsection highlights a few of these. The first major hurdle was with the accuracy of the inverse kinematics system built into Pybullet. The initial inaccuracies were not a bug in the simulation, but a misunderstanding of the signature of some of the functions.
The Franka panda robot arm has 12 individual joints. However, upon closer inspection it becomes apparent that 3 of these joints are not actuated and are fixed in place relative to the previous joint. As such the arm only has 9 degrees of freedom, while reporting all 12 joints. This issue is only apparent when using inverse kinematics to calculate the joint angles to reach a desired end effector position, since this function returns a list of length equal to the degrees of freedom of the arm. This means we get a list of 9 angles instead of 12. The function to control the arm motors, \verb|pybullet.setJointMotorControlArray()| expects one angle for every joint. As such if we pass the list of joint angles directly, we end up trying to move some of the fixed joints, while leaving the top 3 joints unspecified, resulting in not moving them. While this is not difficult to fix, there are numerous inconsistencies in the signature of different functions, which are not clearly documented. Each of these issues slowing down development while it is debugged.\\

Another point of note specific to the simulation is how cameras are implemented. The camera's in Pybullet do not use the standard intrinsic and extrinsic matrix to convert world coordinate to pixel coordinates in the image. Pybullet instead uses a projection matrix and view matrix respectively. These broadly serve the same purpose with the view matrix capturing the position and orientation of the camera and the projection matrix capturing the specifics for how points are projected onto the 2D image. The most notable difference is that the projection and view matricies are $4\times4$ instead of $3\times3$, since they are applied to homogeneous coordinate points \cite{homogenous-coords}. In this paper we will refer to the projection and view matrix as this is what our simulation uses. However, one could easily use a system which instead requires an intrinsic and extrinsic matrix instead.\\

In PyBullet, orientations are represented using quaternions, \cite{quaternions, orientations} an explanation of which is provided in \refsec{sec:rotations}. Despite adding a fourth dimension, quaternions provide a big advantage over the alternative of Euler angles. This being that quaternions do not suffer from \socalled{gimbal lock} \cite{gimbal, gimbal-lock}.

\subsection{Collecting demonstrations}
In order to generate the trajectories needed for the imitation learning system, a utility program is created to streamline the collection of demonstrations. This program generates trajectories by recording key frame poses throughout the demonstration. The demonstration can be copied by matching each of these key frame poses, with interpolation poses between them.
This program allows for the user to control and move the end effector position and orientation in Cartesian world coordinates. Once the arm is positioned in the correct pose, the user can save the current pose as a key frame of the trajectory. From here they can move the arm again and save the next key frame. If a mistake is made, the user can revert the robot arm to the last saved key frame.\\
The trajectory is stored as a list of end effector poses, the reason for which will be discussed in \refsec{sec:trajectories}.

The program is written to interface with a video game controller, so that the user can control the robot using an analogue input method such as the joysticks. This makes the program slightly more user friendly, as moving the joystick only slightly will move the arm very slowly and so allows for finer control.\\
In addition to controlling the robot, the program allows for camera rotation and zooming to better see the full robot pose, and can also enable a wire-frame view, visible in Figure \ref{fig:arm-wire}.
%TODO get new figures in table environment without joint colour coding


\section{Encoding the problem}
\label{sec:trajectories}
In this paper we plan to build upon existing ideas in the field while reducing the barrier to entry formed by using extremely resource intensive AI models. We intend to provide the robot with an example trajectory which successfully accomplishes the skill, in addition to some further context which allows the robot to generalise this trajectory to different environments.\\

Let us formalise the concept of demonstrations and trajectories, as this will be key in understanding how we can transfer a demonstration from the environment it was given in to other unseen environments. Firstly we decompose the state of the system at time t as the current state of the robot and the state of the environment.
$$s_t = (s^{(r)}_t, s^{(e)}_t)$$
We choose to make this separation because the robot state is fully observable and within our control, while the environment is only partially observable and cannot be directly controlled. It can only be influenced by the robot.

\subsection{Robot state}
We have a number of options for how to represent the state of the robot. The most expressive approach is to record the angle of every joint of the robot. This uniquely defines an exact pose of the robot.
\\
Another option is to encode the robot's state as the end effector position and orientation. This option is appealing since for the purposes of performing manipulation tasks, the end effector pose is our main concern. We likely do not care about how the robot achieves this end effector pose, so long as it does. For this reason storing only the end effector pose at each time step, and dynamically computing the joint angles to achieve this using inverse kinematics, appears to be a desirable choice.\\

However it is worth considering that in this case an end effector pose does not uniquely define the entire robot pose. This is because the end effector pose has 3 degrees of freedom for the position and 3 for the orientation (4 quaternion components however the sum of squares must equal 1, tying down the 4th value once 3 are already chosen). In contrast the robot used in this project has 9 degrees of freedom, as discussed in \refsubsec{subsec:pybullet-specifics}. This means that the inverse kinematics system of equations is underdetermined since we have in essence 6 constraints but 9 unknowns to satisfy them with. As such, the inverse kinematics system of equations has infinitely many solutions. What this means for us is that the end effector pose does not uniquely define an entire robot pose. There are in theory infinitely many robot poses which would result in the end effector reaching the position and orientation we wanted it to.\\

\reffig{fig:underdetermined} demonstrates an analogous instance in 2 dimensions for easier viewing. In this system the target position is an (x,y) position and the orientation can be defined by a single angle $\theta$. This means we have 3 degrees of freedom. Therefore, a robot arm with 4 or more controllable joints would create an underdetermined system. \reffig{fig:underdetermined} showcases just 3 possible solutions, but one can imagine how infinitely many may exist. One can also extrapolate this idea into 3 dimensions.

\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/ik-multi-1.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/ik-multi-2.png}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/ik_multi-3.png}
    \end{subfigure}
    \caption{An underdetermined inverse kinematics system has multiple solutions}
    \label{fig:underdetermined}
\end{figure}

While this initially seems like a big problem, for our purposes we truly do only care that the end effector is in the position and orientation we specify. Any of the satisfying robot poses will do. Working with the end effector pose directly also simplifies a lot of our work later on. We will see in \refsec{sec:alignment} that a lot of our calculations will involve the end effector pose. While we could compute this using forward kinematics if the demonstration instead stored joint angles, this would be an unnecessary extra step for no real benefit to the accuracy of the system. A way to use the underdetermined nature of the IK system to our advantage is explored further in \refapx{apx:null-space}.\\

We may optionally decide to also include velocity information in addition to positional information. This would allow us to specify that the robot should have an exact position, but also describe how it should move in the next time step. While this may be useful for very specific tasks like throwing an object to hit an exact target; for the majority of tasks, velocity information is simply not necessary when we have the position for the next key frame already defined. If we were to save a key frame every time step, then the velocity information would be completely redundant, with velocity being directly computable as the displacement between the two key frames divided by the time step. In practice, for usability, we opt to not do this and only record key frames when deemed necessary by the user during the demonstration. While this does mean velocity information is not directly knowable, we decide that velocity information is not necessary for the simple tasks we are experimenting with in this paper.\\

One final thing to consider with how to represent the robot's state is the end effector gripper. Since the gripper arms are attached to the end effector, they are not defined by the inverse kinematics result. Moving the gripper arms has no effect on the pose of the end effector, and so any gripper arm positions would constitute a valid IK solution. Therefore, we need to add some information to our state which uniquely decides where the gripper arms are. We could use the angle of each gripper arm, however it is unlikely we would want the grippers at different angles, so we can simplify to a single angle which we mirror for both grippers. Simplifying even further, we do not anticipate a situation where the gripper would need to be in an intermediate half closed state. We either want the gripper fully closed, applying a constant pressure to the item it is holding, so that the item does not fall out of its grasp; or we want the gripper fully open, so that it can drop the item it is holding. As such we can see that a single boolean value is sufficient to define the gripper state in our situation.
$$g \in \{0,1\}$$

With these decisions in place we can define the robots state as an 8 vector, containing the position and orientation (as a quaternion) of the end effector, along with the gripper state. 
$$s^{(r)} = [x_{eef}, y_{eef}, z_{eef}, a_{eef}, b_{eef}, c_{eef}, d_{eef}, g]$$

\subsection{Environment State}
We define the state of objects similarly to how we defined the end effector pose storing the 3D position and orientation of the object within the scene. Objects do not have an associated gripper, so an objects state is a 7 vector. The state of the whole environment is just the collection of all the object states within the scene.
$$s^{(e)} = \{[x_o, y_o, z_o, a_o, b_o, c_o, d_o]\}_{o=1}^O$$

We note that without a perfect model of the environment, the environment's state is not fully observable. In implementation we will not be able to capture the full environment state and instead choose to represent it differently entirely. For example capturing the environment state as an image of the scene using methods similar to those employed by P. Vitiello, K. Dreczkowski, and E. Johns \cite{one-shot-pose-estimate}. For now we will continue in the purely theoretical sense, where the environment state can be fully observed.\\

\subsection{Trajectories and Demonstrations}
Given that we have defined the state of the system, a naive approach would be to define a trajectory as the list of states traversed at each time step of the trajectory.
$$\tau = \{s_t\}_{t=1}^T$$
We can now clearly see that a trajectory is a large $T \times D$ matrix where $D$ is the dimensionality of the state. $D = 8 + 7 \cdot O$ in this idealised setting.
$$\tau \in \real^{T \times D}$$

This definition is naive for a number of reasons. Firstly we can see that a trajectory defined in this manor will include a large amount of unnecessary information. In completing a task, we will likely only need to move the robot arm to a few key positions to ensure the task succeeds. However this current definition saves a pose at every time step. We can reduce the dimensionality of a trajectory substantially if instead of saving the robot's state at every time step, we only save the relevant key frames. When executing this trajectory we will then interpolate the poses between each key frame, to recover the pose at each time step. We can now add as many key frames as are required to fully express how to complete the task, without including unnecessary in between poses. We denote a trajectory as having $K$ key frames.\\

Another issue with the current trajectory definition is that in addition to the environment state not being fully observable, it is not directly within our control. We cannot choose to change the state of the environment by magically moving an object. We can only influence the environment through the actions of the robot. As such, this approach still includes a large amount of irrelevant information. Furthermore, we want a trajectory to encode a skill. The necessary steps the robot needs to take to complete some task. This task can be completed regardless of the specific environment state and so the environment variables should not be a part of the trajectory. Conceptually, the trajectory encapsulates a skill independent of the environment it is within. The notion of influencing the trajectory based on the environment is considered more a transformation applied to the trajectory, than a component of the trajectory itself.\\

This is an important difference in definition to a demonstration. A demonstration is a human provided trajectory accompanied with some additional environment information. This additional information is what allows us to compare scenes and transform the trajectory. The trajectory itself does not include any environment specific information.\\

With these new definitions in mind we can see the trajectory is a $K \times D$ matrix where K is the number of key frame poses and $D$ is only the dimensionality of the robot's state, $D = 8$.
$$\tau = \{s^{(r)}_t\}_{k=1}^K$$

A demonstration is a trajectory combined with some environment information, which for now we denote as $\Sigma$. $\Sigma$ must correspond to the exact environment in which $\tau$ was presented by the human.
$$\Delta = [\tau, \Sigma]$$

\subsection{Environment context}
There are numerous ways we could choose to define $\Sigma$, each with their own benefits and drawbacks. The purpose of $\Sigma$ is to provide some context as to how the world looked when the trajectory was given. We will then compare this to the environment at test time, and modify the trajectory into the new scene.
$$\tau' = f(\tau, \Sigma, \Sigma')$$
Where $f$ is the main goal of this project. Some function, which we call the \socalled{trajectory transfer function}, capable of mapping the trajectory from the original environment to the new environment. How $f$ is defined and implemented is explored thoroughly in \refsec{sec:implementation}.\\

If the environment were fully observable, then an easy choice for $\Sigma$ would be the initial environment state.
$$\Sigma = s^{(e)}_{t=0}$$
This perfectly captures what the environment looked like when the trajectory was given by the human. It encodes all the information we need which justifies why the human provided this exact trajectory to complete the task. Note we only need to store the initial environment state, there is no need to encode the state across all time steps. This is because we assume that the optimal method to complete a task is apparent from a single glance at where all the objects within the scene are. We assume the environment is static, and can only be influenced by the robot's actions itself. In this sense the robot and environment are a closed system. No external actors can influence the environment. If external actors could influence the environment then a single initial snapshot would no longer be sufficient.\\

This closed-system property is crucial even in the partially observable case. When the environment state is not fully observable, and we cannot know $s^{(e)}$ perfectly, we must make an observation to approximate it. As before, we conclude that if no external forces can be applied, then a single observation is enough. We just need to make sure that our observation is rich enough to infer all the information we need about the environment state.\\

For this project, we utilise methods presented by N. Di Palo and E. Johns in their implementation of \socalled{Dinobot} \cite{one-shot-imitation} to form the basis of our environment context. In their paper they use an RGB-D image taken prior to the trajectory being given. This satisfies the requirement of an observation made at time $t=0$. From this image, we are able to infer the position of objects within the image. Dinobot uses a first person, wrist mounted camera to capture the RGB-D image. This is in contrast to other papers which use a fixed third person camera \cite{one-shot-pose-estimate}.
It is important that the object of interest which we will be interacting with is within the image, as the end effector will need to align relative to this object. If the object is not visible within the picture, then the robot will align relative to a different object it can see. Unless this object and the correct object are in the exact same position relative to each other, then this would cause the task to fail. 
While a third person camera capturing the entire environment would guarantee the object of interest is visible, using a wrist mounted camera leads to lots of nice properties discussed in \refsec{sec:alignment}. In their paper, N. Di Palo and E. Johns combat this problem by placing the robot in a known initial pose with the end effector and attached camera placed high pointing down onto the scene. This captures a wide view which should encompass any relevant objects.\\

This however enforces a restriction that the demonstration must always be given from the same initial starting position. This may not be an issue if the initial position is chosen well, but it is possible to create an environment where important information is occluded when viewed in this initial position. Consider \reffig{fig:occluded}. The task is to pick up the block from underneath the bridge. However, from the initial position in red, the robot is unable to see the object of interest, making it impossible to align correctly. The green line shows an alternative initial position where the object is no longer occluded.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/occluded-no-bg.jpg}
    \caption{Example environment where initial position cannot view the object of interest}
    \label{fig:occluded}
\end{figure}

Whatever initial pose we choose for the robot, we could construct an example where the object of interest is not visible. The solution to this is to not fix the initial pose, but allow it to be dynamically chosen by the human giving the demonstration. All we need to do is include the initial pose in the environment context, along with the image it captures. Now rather than the initial pose being fixed in the code, we can use the initial pose defined in the demonstration instead.
$$\Sigma = [I^{(RGB-D)}, s^{(r)}_{t=0}]$$

In practice we make a few modifications to simplify the implementation. As will be seen in \refsec{sec:alignment}, the purpose of knowing the initial pose is to know the extrinsics of the camera which took the image. If we know the position and orientation of the camera, we can compute world coordinates from the pixel coordinates of objects in the image. As such we decide to store the extrinsics directly, instead of computing them from the initial pose. As mentioned in \refsubsec{subsec:pybullet-specifics}, in Pybullet the extrinsics of the camera are captured by the view matrix, which we denote $V$.\\

The other implementation difference is to store the depth information separately to the RGB data. This is because the RGB pixels are integers between 0 and 255, while the depth information is stored in \socalled{Normalised Device Coordinates} \cite{ndc}. These are float values in the range 0 to 1 where 0 represents an object on the near clipping plane and 1 represents an object on the far clipping plane. Mapping the depth information into integers of the range 0-255 and including as a 4th channel of the RGB image proved to cause too much loss in precision when recovering the true values, leading to very poor performance.\\

With these differences in mind we can define the final form of the demonstrations:
$$I^{(RGB)} \in (\nat_{[0,255]})^{H \times W \times 3}, \longspace
I^{(D)} \in (\real_{[0,1]})^{H \times W}, \longspace
V \in \real^{4 \times 4}$$
$$\Sigma = [I^{(RGB)}, I^{(D)}, V]$$
$$\Delta = [\tau, I^{(RGB)}, I^{(D)}, V]$$
Where $W$ and $H$ are the width and height of the image captured. These values are not too important provided they remain constant for all demonstrations. In this paper we choose $W=H=1024$ pixels.


%TODO im not sold on the section title
\section{Implementation Overview}
\label{sec:implementation}

\reffig{fig:pipeline} shows an overview of the pipeline developed in this paper. The first step of the algorithm is to select the demonstration to follow. The robot will look at every demonstration image in its training corpus, and select the one which is most similar to the current environment. It will then compare this demonstration image to the live image taken by the robot's camera. From these two images LiteBot then extracts keypoints, and uses these to compute the transformation which has been applied to the object. This offset is then applied to the end effector, transforming the demonstration trajectory into this new environment. The robot can then execute this transformed trajectory, completing the task. The following sections detail the specifics of each module of the pipeline.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/pipeline.png}
    \caption{Overview of system pipeline}
    \label{fig:pipeline}
\end{figure}

\section{Demonstration Selection}
\label{sec:demo-selection}
The first step of implementation comes before we can even consider the trajectory transfer function. We first need to consider its inputs. $f$ takes as parameter the task trajectory $\tau$, the original environment context $\Sigma$ and the context of the new test time environment $\Sigma'$. While $\Sigma'$ can be directly observed from the test time environment, $\tau$ and $\Sigma$ are dependent on the demonstration we choose to follow.\\

We want LiteBot to be able to learn multiple tasks independent of each other and apply the correct task to a given scene. We do not want the human to have to tell the robot what it should do with the objects. The robot should be able to infer the task from the objects alone. More specifically, from the environment observation $\Sigma'$.
The robot will have access to a pre-trained corpus of demonstrations. The \socalled{One-shot} nature of this system refers to each task only requiring a single demonstration to be learned. However, to teach multiple tasks, a new demonstration for each task is still required. Each of these demonstrations has associated with it, the environment context $\Sigma$. Deciding which task to complete in the new environment is a matter of comparing the context image of each demonstration, to the live image the robot takes upon being presented with the environment. For this purpose, only the RGB image is used, the depth information and view matrix are ignored for now.\\

In order to compare the images, N. Di Palo and E. Johns opt to embed both images using a vision transformer created by Meta (formerly known as Facebook) called \socalled{Dino} \cite{dino-paper}. The transformer produces a 768-vector, and we can directly compare the embedding vector of different images using cosine similarity. Whichever embedding has highest similarity to the live embedding, corresponds to the context image being most similar to the live image. The hope is that the transformer extracts useful information regarding the relevant parts of the image, such as the general make-up of objects within the scene, while ignoring less important information like the positional information of objects.\\

We can expect the similarity between the embeddings to be greatest when the corresponding images contain the same objects. However, this system naturally extends to generalising to unseen objects as well. If no such demonstration exists on the specific object in the live scene, then we can still select whichever demonstration is closest. Since we directly compare not the images themselves, but the image embeddings from the vision transformer, this translates to selecting the image with the object which the transformer sees as the most similar. For example the robot had a demonstration to pick up a can, it is likely that when faced with a bottle in the current scene that the system would select the demonstration of the can. This is because the objects appear similar as they are both cylindrical items, and the robot would have to interact with them in similar ways, by grasping around the cylinder. As such we can conclude that the demonstration of the can is the closest one to working with this new object. The hope would be that these two objects are similar enough that the same demonstration would be able to successfully complete the task on this new object.


\section{Trajectory Transfer}
\label{sec:alignment}
Now that we have the necessary parameters, we can begin implementing the trajectory transfer function, $f(\tau, \Sigma, \Sigma')$. In the previous subsection we found a demonstration which shows us how to manipulate the object in the scene (or a similar one). Now we need to consider the pose of the object to transform the trajectory. During the demonstration, the object was at some position with some orientation in world space $\{X_{demo}, \Theta_{demo}\}$. When the object is at this exact position and orientation, we know how to complete the task by executing the demonstration trajectory, $\tau$. In effect we know that for the specific case where the environment has not changed:
$$f(\tau, \Sigma, \Sigma) = \tau$$

However, generally in the live scene the object may have moved to some different position and orientation $\{X_{live}, \Theta_{live}\}$. We define the offset between the two poses as the translation and rotation which maps the demonstration pose onto the live pose. Formally \footnote{For an explanation of the formula for R, refer to \refsec{sec:rotations}}:
$$M = X_{live} - X_{demo} \longspace R = \Theta_{live} \Theta_{demo}^{-1}$$ 

We need to modify the trajectory to account for this offset, getting $\tau'$. Recall that $\tau$ is just a list of end effector positions and orientations through time. So to calculate $\tau'$ we add the translation and rotation offset to each pose of the trajectory.
$$\tau = \{P, O\}_{t=1}^T \longspace \tau' = \{P + M, RO\}_{t=1}^T$$

Since we are currently working in a simulation, the environment state is fully observable. We can query the simulation to get the exact position and orientation of the object of interest during the demonstration and record this along with the demonstration trace. We can then compare this to the environment state in the live scenario and compute the exact offset between the two object's and adjust the demonstration trace by this amount.

However, this information is only afforded to us because we are working in a simulation. We want our solution to be deployable onto real world robots without further modifications. In a real world implementation we would not be able to perfectly know where the object is. We only have access to the observations made by the camera attached to the robots end effector. As such we limit ourselves to a partially observable environment and use the observations alone for our calculations. We will use the observations to produce an estimate of the object's true position and orientation, using this in place of the true value which we no longer have access to. This is why the trajectory transfer function takes as parameter observations of the environment $\Sigma$ and $\Sigma'$, not the true environment state $s^{(e)}_{t=0}$ and $s'^{(e)}_{t=0}$.\\

We note that while we chose to use an estimate of the object's world coordinates to compute the offset, this is not absolutely necessary. Our true goal is to align the end effector so that relative to the object, it is in the exact same location as in the demonstration. Once this is the case we can compute the same offset as before, but this time using the end effector's position and orientation. We denote the end effector's pose in world coordinates at the start of the demonstration, and during the live scene once alignment has completed as $\{E_{demo}, \Phi_{demo}\}$ and $\{E_{live}, \Phi_{live}\}$ respectively. Given this then we can compute M and R without requiring the position of the object at all.
$$M = E_{live} - E_{demo} \longspace R = \Phi_{live} \Phi_{demo}^{-1}$$

It is important to remember that here $E_{live}$ is the end effector position only once it has aligned to the object. Formally, $E_{live}$ is the end effector position in world coordinates, such that $E_{live}$ and $E_{demo}$ are equivalent when viewed from the reference frame of the object in the live and demo scene respectively.
$$E_{live}^{(obj_{live})} = E_{demo}^{(obj_{demo})}$$

By using this approach we completely remove the need to estimate the object's position and orientation. We only need to compute the end effector's pose, which is trivial using forward kinematics. Furthermore it is relatively easy to decide when the end effector is aligned, since we can just compare the positions in the object reference frame. We know that if the end effector is in the same position relative to the object, then the object is in the same position relative to the end effector. The converse statement is also true.
$$E_{live}^{(obj_{live})} = E_{demo}^{(obj_{demo})} \iff X_{live}^{(eef_{live})} = X_{demo}^{(eef_{demo})}$$

This is why using a wrist mounted camera is useful. Since the camera is attached to the end effector, if the object is in the same position relative to the end effector, it will be in the same position in camera space. And if this is the case it will be in the same position in image space. This means that if the object is in the same position relative to the end effector, it will be in the exact same position in the image taken by the camera.
$$X_{live}^{(eef_{live})} = X_{demo}^{(eef_{demo})} \implies X_{live}^{(image_{live})} = X_{demo}^{(image_{demo})}$$
%TODO: this one isnt iff since the object could move away and be smaller but in the same position. This is because X^(img) is only 2D unlike the real world coordinates. Write a short paragraph explaining this. Maybe if we pretend the image is RGB-D and not stored seperatly, then it becomes iff, think about it.

If this is the case then the image captured in the demonstration and live scene will be exactly the same (barring any background noise such as extra objects). This makes it easy to detect when the end effector is aligned since we just move until the end effector until the live and demonstration image match within a suitably low tolerance. The downside to this method will be more clear after discussing \refsec{sec:kabsch}. While we can still compute the difference between the position and orientation of the object in image space, this results in a translation in a unit of pixels rather than meters which is used by the simulation. This means that while it is easy to know when the end effector has aligned, it is difficult to know by how much we should move to align. This method also makes it much more cumbersome to include depth information, since the depth data is not computed in a unit of pixels, but rather holds the actual depth from the camera to the object in normalised device coordinates. As a result, the x and y coordinates of the object have a different unit to the z coordinate. While these are all issues which are theoretically solvable, it is much easier and debatably more elegant to perform all the calculations in a single reference frame, by estimating the pose of the object in world space. As such this will be the method used going forward.


\section{Using keypoints to approximate position}
\label{sec:keypoints}
As previously mentioned, our goal is to align the live and demonstration image so the object looks the same relative to the end effector camera. It is difficult to work with the image directly, since working with the colours of pixels leaves the system sensitive to lighting conditions and noise in the image. Instead we would like to use the image to estimate the position and orientation of the object in world space, and use this for our calculations instead. One approach could be to identify the bounds of the object using computer vision techniques, to work out exactly which pixels represent the object. This is relatively simple since the object is almost certainly a different colour to the background. By knowing which pixels belong to the object we can estimate the centre of the object and use this as the position of the object in image space.\\

\textit{For all figures in this chapter we will assume the left image is the live image and the right image is the demonstration image.}\\

\reffig{fig:centre-of-mass} shows how the centre of mass in the live image is further left than it was in the demonstration image. Therefore the object is too far to the left compared to the demonstration. To fix this we would want to move the end effector camera to the left, so the centre of mass in the live image would move to the right, aligning with the demonstration image.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ideal_centre-of-mass.png}
    \caption{Centre of mass matching}
    \label{fig:centre-of-mass}
\end{figure}

However, note that while a single point is enough to compute the translation offset between the object in different scenes, it is not sufficient to compute the orientation offset. By reducing the object in the image to a single point, we have estimated its position, but have no information about its orientation. This is a problem for us since we want the system to be robust to not only moving the object, but also rotating it. Some objects may behave very differently in different orientations if they do not have rotational symmetry. As such we need a way to estimate the orientation of the object in both images.\\

In order to achieve this we cannot reduce the object to a single central point, but we can reduce it to a collection of points. Having multiple points which match in the two images can allow us to identify if a translation or rotation have occurred. \reffig{fig:ideal-matches} shows how an ideal set of matches allows us to infer both a translation and rotation between the object in the images. In this case we can see the block needs to move to the right of the frame, but now also needs to rotate approximately 45$\degrees$ anticlockwise (a positive rotation according to the right hand rule) about the vertical axis.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ideal_matches.png}
    \caption{Ideal keypoint matching}
    \label{fig:ideal-matches}
\end{figure}

Note that this is only one way the keypoints could be matched. \reffig{fig:ideal-matches-inv} shows another way we may choose to match the keypoints. In this case the live object should move to the right and rotate approximately 45$\degrees$ clockwise (a negative rotation according to the right hand rule) about the vertical axis, in order to align with the demonstration object. While having multiple possible ways to match the keypoints may seem like a problem, this is merely a consequence of the object in question having rotational symmetry. It does not matter which way we rotate the object, because it will behave the same, whichever side we are facing, and each side is indistinguishable from each other. As such, for our purposes it would not matter which matching and subsequent transformation we chose, since in either case the object is aligned to the demonstration image. In practice this situation would not likely occur, since we will have many more keypoints, and the keypoints will very likely not be evenly spaced as in this ideal example.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/ideal_matches_inv.png}
    \caption{Alternative ideal keypoint matching}
    \label{fig:ideal-matches-inv}
\end{figure}

We can also see that in both figures, the live image contains two additional keypoints marked in red. These keypoints could not be matched to any in the demonstration image so are ignored when calculating the translation and rotation to align the objects. In this case it is because these keypoints become occluded when we view the object in the demonstration image.\\

These figures are designed to give an intuition for how we can use keypoints to find the desired end effector offset. In these figures we have considered ideal keypoints where the keypoints are extracted on useful points of the image, namely the object corners, and where the matching is perfect with no mistakes. These keypoints have been placed by a human for the purposes of demonstration. Obviously for the complete system we want the keypoint extraction and matching to be automated. How we achieve this will be explored next.


\section{Automated keypoint algorithms}
\label{sec:keypoint-algos}
Keypoint extraction involves identifying distinctive points or features in an image that can be reliably recognized under different viewing conditions, such as taking the same image from a different angle, zoom, or changing the lighting conditions. These keypoints, often represent corners or edges of objects within the image, since these are easy to recognize across different images.
Once we have chosen the keypoints we need some way to identify them. This is achieved through creating feature descriptors, which provide a unique signature for each keypoint based on the image information in the local area around the keypoint. These feature descriptors should be invariant to the viewing conditions described above, so that the same keypoint is assigned the same (or as close as possible within some error) descriptor, even when viewed from different angles, zoom, or lighting conditions.
Once keypoints are extracted from different images, keypoint matching techniques allow us to find correspondences between the keypoints in two different images of the same scene. This section explores various keypoint extraction algorithms, how they work, and resulting advantages and disadvantages for our use case. As well as this we discuss the methods used to match these keypoints across images.\\

\subsection{Keypoint Extraction}
Most keypoint detection algorithms work by identifying points in the image where it sharply changes. These are often the boundaries between an object and the background. Mathematically, these are the points in the image where the derivative peaks (either positively or negatively). The derivative is approximated by the finite difference between pixel intensities, and computed in both the X and Y direction of the image. We can therefore compute the magnitude and direction of the gradient:
$$|g| = \sqrt{g^2_x + g^2_y}, \longspace \theta = \arctan{\frac{g_y}{g_x}}$$
where $g_x$ and $g_y$ represent the finite difference (gradient) in the X and Y direction respectively. 

\subsubsection{SIFT}
The first keypoint detection algorithm we look at is Scale-invariant Feature Transform (SIFT), designed by David Lowe and published in 2004 \cite{SIFT}.\\

The extraction process begins with identifying potential keypoints through scale-space extrema detection. This involves creating a series of progressively blurred images using Gaussian filters at different scales and computing the \socalled{difference of Gaussians} (DoG) to highlight areas of interest. Local extrema in these images are points which stand out from their neighbours in both space and size. The keypoints are further localized to sub-pixel accuracy.
%TODO maybe say how DOG aproximates laplacian

Once keypoints are identified we must assign a descriptor. To ensure the descriptors are invariant to image rotation, all local pixels vote for a dominant orientation in a discretised histogram using their gradient orientation. The winning bucket is the dominant orientation. This dominant orientation is used to offset all future orientation calculations. The local area is now split into a 4x4 grid of sub-regions. Each sub-region produces a histogram of gradient orientations, relative to the dominant orientation, with votes weighted by the gradient magnitude. Each histogram is encoded as the height of each bucket in a vector. All histogram vectors are concatenated creating a final 128-vector descriptor.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/SIFT.png}
    \caption{SIFT keypoint descriptor \cite{SIFT}}
    \label{fig:SIFT}
\end{figure}

SIFT was a seminal algorithm in computer vision which has been tried and tested since its patent expired. It stands as an excellent baseline for our keypoint extraction.


\subsubsection{ORB}
ORB (Oriented FAST and Rotated BRIEF) is a keypoint extraction and description algorithm developed by Ethan Rublee et al. in 2011 \cite{ORB}. ORB combines the FAST (Features from Accelerated Segment Test) keypoint detector \cite{FAST} with the BRIEF (Binary Robust Independent Elementary Features) keypoint descriptor \cite{BRIEF}.\\

FAST identifies keypoints by examining the intensity of pixels in a circular region around each candidate pixel and classifying it as a keypoint if it has a sufficient number of neighbouring pixels that are significantly brighter or darker than the candidate. ORB utilises FAST at different scales to create scale invariant features.\\

Additionally, ORB assigns an orientation to each keypoint as the direction of the vector from the keypoint to the centroid of the intensities within a local patch. This orientation assignment ensures that the keypoints are invariant to image rotation.
BRIEF is then used to assign a descriptor to each keypoint. BRIEF generates a binary string for each keypoint by performing a series of binary intensity difference tests between pairs of pixels at predefined locations relative to the keypoint. \reffig{fig:ORB} shows an example of which pixel intensities may be compared to each other. This results in a 256 bit binary string, as opposed to a 128-vector of floating point values with SIFT.\\

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ORB.png}
    \caption{Example ORB pixel comparison locations \cite{BRIEF}}
    \label{fig:ORB}
\end{figure}

ORB's main advantage is its speed and efficiency. Large amounts of keypoints can be detected and described very quickly, since the majority of calculations are binary tests on pixel intensities.\\


\subsection{Keypoint Matching}
Once we have extracted keypoints and their descriptors, we need a way to find which keypoints represent the same semantic features across the two images. This is done by defining some notion of distance between keypoint descriptors. A shorter distance means the descriptors are more similar, and they are more likely to refer to the same feature of the image.\\

\subsubsection{Brute Force Matching}
The simplest method for keypoint matching is brute force matching. In this approach, each descriptor from one image is compared with all the descriptors from another image to find the best matches. This involves calculating the distance between each pair of descriptors using a suitable metric. For SIFT descriptors this would be the Euclidean distance between the vector descriptors. For ORB this would be the Hamming distance between the bit string descriptors. The descriptor pairs with the smallest distances are considered as potential matches.\\

Brute force matching can be computationally expensive, especially for large datasets, as it requires a comparison of every possible pair of descriptors, resulting in $O(n^2)$ time complexity. The advantage is that this method is highly accurate and does not rely on any approximations, making it a reliable choice. To improve accuracy we can include additional considerations such as cross checking, where matches are verified by ensuring mutual nearest neighbors, and ratio tests, which filter out ambiguous matches by comparing the distance of the closest match to the second closest match.

\subsubsection{Grid-based Motion Statistics}
Grid-based Motion Statistics (GMS) is a more sophisticated algorithm developed by Jiawang Bian et al. in 2017 \cite{GMS}. This paper describes a method for refining keypoint matches leveraging a few key assumptions. If we assume that the two images are of the same object, then we can impose smoothness constraints. Unlike brute force matching, which relies solely on descriptor similarity, GMS incorporates spatial information to filter out false matches effectively.\\

The key principle of GMS is to divide the image into a grid and analyze the distribution of keypoint matches within each grid cell, ensuring that matches are not only similar in descriptor space but also exhibit consistent motion patterns across the image. The object may have been moved, rotated, zoomed and warped by camera projection, but if it is the same object then there will be consistencies in how near by keypoints are transformed.\\

This is perfect for our use case since provided the demonstration selection stage has worked as intended, the object will be the same in both images. Furthermore, the assumption GMS relies is one we have already taken. We have already assumed that the object in both images has simply been translated and rotated between the live and demonstration image. We already mentioned in \refsec{sec:keypoints} that we are using these keypoints to try to recover this transformation. As such, by using GMS we filter out keypoint matches which specifically contradict this assumption, leading to much more stable results when computing the transformations. This is demonstrated in \refchap{chap:evaluation}.

\subsubsection{Outlier Filtering}
While GMS is very good at computing consistent keypoint matches, it still can have some difficulty with large groups of inconsistent matches, since GMS only filters out matches which disagree with its local neighbours. This is great as a first pass and serves to remove the majority of high frequency noisy matches. However, the grid based structure of GMS, means that individual grid cells can often disagree with each other. This is low frequency noise, where collections of matches disagree with other collections.\\

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/outliers-gms.png}
        \caption{Keypoint matches using GMS}
        \label{fig:outliers-gms}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/outliers-gms+ours.png}
        \caption{Keypoint matches using GMS + outlier filtering}
        \label{fig:outliers-gms+filter}
    \end{subfigure}
    \caption{Our method further improves the consistency of keypoints}
    \label{fig:consensus-filtering}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/bad-matches.png}
    \caption{Identifying outliers by their distance from the mean vector}
    \label{fig:bad-matches}
\end{figure}

\reffig{fig:consensus-filtering} shows example keypoint matching using GMS, before and after applying outlier filtering. We can see that in \reffig{fig:outliers-gms}, there is a collection of keypoints which match incorrectly. The object has moved diagonally, as most matches correctly identify. However, this collection correspond to a more horizontal translation. Our filtering method successfully removes these incorrect matches, while retaining the correct matches.\\

In this paper we develop an additional measure to improve our keypoint matches. We further eliminate outliers to refine keypoint matches by considering all matches, not just those within a grid cell. After applying GMS to obtain an initial set of matches, this algorithm performs an additional filtering step based on vector analysis of the keypoint coordinates. We start by considering keypoint matches as vectors within image space. If a keypoint at $(x,y)$ in image one, matches to a keypoint at $(x',y')$ in image two, then we consider the match as the vector from $(x,y)$ to $(x'y')$. This is the line you would get by connecting the keypoints if the two images were superimposed on top of each other. By considering keypoint matches as vectors in image space, we can consider these vectors as describing how the object moved between the images. This is not a direct calculation of the translation and rotation, since we make no assumption that the camera which took the images was in the same pose in world space. However, we are still able to perform some filtering on these vectors regardless.\\

To identify and remove outliers, we examine the gradient of each vector relative to the mean values. Matches that deviate significantly from the mean are considered potential outliers. Specifically, we order the matches based on how much they differ from the mean vector. We then compute the finite difference between consecutive ordered pairs of match vectors. This allows us to see how quickly the derivative changes. The algorithm looks for spikes in the derivative, which indicate sudden changes in the gradient that are inconsistent with the overall motion pattern. We record the index of all points where the gradient spikes more than some hyperparameter threshold. We then check each spike from left to right ensuring that if we were to split here, we wouldn't remove too many keypoints. The justification for this additional test is that if the potential outliers held a majority or close to it, then we would conclude that these points are not in fact outliers. If a potential split would remove too many keypoints, but there are later spikes, then we check each spike until the first one passes. Only once a spike passes both tests do we split the list at this point, and discard the matches with a mean distance higher than that of the split point. \reffig{fig:bad-matches} shows a plot of distance from the mean and the finite difference for the images shown in \reffig{fig:consensus-filtering}\\

\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/inliers-gms.png}
        \caption{Keypoint matches using GMS}
        \label{fig:inliers-gms}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \includegraphics[width=\textwidth]{figures/inliers-gms+ours.png}
        \caption{Keypoint matches using GMS + outlier filtering}
        \label{fig:inliers-gms+filter}
    \end{subfigure}
    \caption{Our method only removes outlier matches}
    \label{fig:consensus-filtering2}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/good-matches.png}
    \caption{A rotated object will have a smooth increase in distance}
    \label{fig:good-matches}
\end{figure}

It is important to analyse the derivative and not just remove the vectors furthest from the mean. This is because if a rotation is present then the vectors will not agree perfectly on their orientation and magnitude. However, these changes should be smooth in the ordered list of matches, since a rotation would affect all vectors proportional to their distance from the centre of rotation. These matches are not outliers and should not be removed. As such, only when the finite difference changes drastically can we accuse the matches of being outliers. \reffig{fig:consensus-filtering2} shows an object which has been rotated. For this object the matches are very accurate and consistent. Outlier filtering removes very few matches as outliers. We can see in \reffig{fig:good-matches} that this corresponds to a smooth increase in the distance from the mean. As such the derivative is much more stable.\\

In \refchap{chap:evaluation} we compare the effectiveness of each keypoint algorithm with each matching algorithm, to determine the best combination of algorithms for use in this project.\\

%TODO these figures are being placed in the next section. Not ideal, try to fix
% it kinda fixed, check before final submission


\section{Calculating end effector offset}
\label{sec:kabsch}
Now that we have a list of matched keypoint pairs, we can begin to calculate the offset between the object in the live and demonstration image. Fortunately there exists an algorithm for computing the translation, rotation and scaling, which best maps a set of points onto another set of points. This algorithm is called the \socalled{Kabsch-Umeyama algorithm} \cite{kabsch}. The algorithm takes in two sets of points, a reference set $P$, and a test set $Q$. It computes a translation $t$, a rotation matrix $R$ and a scale factor $s$, such that when these transformations are applied to $Q$, yielding $Q'$, the root mean squared distance between $P$ and $Q'$ is minimized. For our purposes we make a number of important modifications to the original algorithm to better fit our use case.\\

As mentioned above, we decide to convert all keypoints to world coordinates first. By doing this we can more easily include the depth information of the image. Additionally, this will result in the translation computed already being in world coordinate units, meaning no further calculations are needed to compute how far to move the end effector.\\

\begin{algorithm}
    \setlength{\baselineskip}{18pt}
    \caption{\textbf{Kabsch-Umeyama algorithm}}
    \label{alg:kabsch}
    \begin{algorithmic}[1]
        \Require $P = [p_1, p_2, \dots p_N], \shortspace p_i \in \real^M$
        \Statex $Q = [q_1, q_2, \dots q_N], \shortspace q_i \in \real^M$
        \Ensure $|P| = |Q| = N$
        \Procedure{Kabsch-Umeyama}{P, Q}
            \State $\overline{p} \gets \frac{1}{N}\sum^N_{i} p_i$
            \State $\overline{q} \gets \frac{1}{N}\sum^N_{i} q_i$
            \State $\sigma^2_P \gets \frac{1}{N}\sum^N_{i} |p_i - \overline{p}|^2$
            \State $H \gets \frac{1}{N}\sum^N_{i} (p_i - \overline{p})^T(q_i - \overline{q})$ \Comment{Covariance Matrix}
            \State $U, \Sigma, V^T \gets \Call{SVD}{H}$ \Comment{Singular Value Decomposition}
            \State $c \gets \frac{\sigma^2_P}{trace(\Sigma)}$
            \State $R \gets U \cdot V^T$
            \State $t \gets \overline{p} - cR \cdot \overline{q} $
            \For{$i=1 \dots N$}
                \State $q_i' \gets t + cR \cdot q_i$
            \EndFor
            \State \Output $t, R, c, Q'$
        \EndProcedure
        
    \end{algorithmic}
\end{algorithm}

\refalg{alg:kabsch} shows the standard Kabsch-Umeyama algorithm. The first step of the algorithm will look familiar to something we tried in \refsec{sec:keypoints}. The first step is to compute the centroids of the two sets of points. This is just the average position of all the points in the set. This acts like the centre of mass of the point set. With sufficiently many and well spaced keypoints in our set, this centre of mass will be approximately equal to the centre of mass we computed earlier by considering all pixels of the object. The algorithm then normalises the points so that the centre of mass is at the origin. From here it computes the covariance matrix, H, between the two sets of points. This covariance matrix tells us how the X, Y and Z coordinates vary between the two sets of points.\\

Next we perform singular value decomposition (SVD) to break H into a composition of 3 elementary transformations.\\

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/SVD.jpg}
    \caption{Singular Value Decomposition}
    \label{fig:svd}
\end{figure}

In \reffig{fig:svd} we can see that the effect of multiplying by the matrix M is to first rotate by the matrix $V^T$, followed by a scaling by the diagonal matrix $\Sigma$, followed by another rotation by the matrix $U$. We note that $\Sigma$ is diagonal, where each element defines how much to scale in each direction. In this figure, we scale by $\sigma_1$ in the X direction and $\sigma_2$ in the Y direction. Therefore
$$\Sigma = \begin{bmatrix}
  \sigma_1 & 0\\
  0 & \sigma_2
\end{bmatrix} $$

This notion naturally extends to higher dimensions. Since $U$, $\Sigma$, $V^T$ all have the same shape as the input matrix M, when we pass in our $3 \times 3$ covariance matrix, we get the rotation, scaling and rotation in 3 dimensions. Note that $\Sigma$ is the conventional notation for this diagonal matrix, it is not the demonstration environment context which we also denoted with capital sigma.\\

From here we compute the rotation between the points as $R = U \cdot V^T$. This is just the combination of the rotation parts of the covariance matrix H. 
The scale factor is computed from the standard deviation of points within the reference set P, normalised by the amount of scaling caused by $\Sigma$ in the covariance matrix decomposition. The translation is then computed as the difference between the centroids after the rotation and scaling transformation is applied.\\

%TODO maybe a better segway sentence
We make a number of modifications to the algorithms which reflect specifics of our use case. The first major difference is in computing the rotation matrix. As can be seen in line 10 of \refalg{alg:modified-kabsch}, we include an additional matrix S in the construction of R. The purpose of this is to prevent reflections in our transformation. In the original Kabsch-Umeyama algorithm, any transformation to map Q onto P is allowed. This includes translation, rotation and scaling, but it also implicitly includes reflections. The reflection is not explicitly computed, rather it is hidden away inside the rotation matrix R. In the original algorithm it is possible for R to have a determinant of -1. If this is the case then the matrix actually represents a rotation and reflection combined. For our purposes we do not want to allow reflections since we cannot reflect the object by moving the end effector, we can only rotate around it. In order to prevent reflections we just need to negate the last row of $V^T$ (or the last column of U) before computing the dot product to find R. To avoid needing to recompute R if we notice it has a negative determinant, we can find the determinant before we compute R, using the fact that $det(A \cdot B) = det(A) \cdot det(B)$. If this determinant product equals $-1$ then the determinant of R will be $-1$ and so we need to negate the last row of V. This is achieved with the matrix S. This matrix will be set so that it is the identity matrix if the determinant of R is already going to be $1$, having no effect. But if the determinant of R is going to be $-1$, then we set the bottom right element of S to be $-1$, which in the dot product calculation will have the effect of negating the last row of V.\\

Another noticeable difference between \refalg{alg:kabsch} and \refalg{alg:modified-kabsch} is that in our version we do not need to compute the scale factor c or the transformed points Q'. We do not compute Q' simply because we do not need these points, we just want the transformation so that we can apply it to the end effector. However, the reason we don't compute a scale factor is more subtle. It is the same reason we do not allow reflections in the rotation matrix, we cannot magically scale points by moving the end effector. The scale factor would allow us to scale all points in Q further from $\overline{q}$. However, this would correspond to physically making the object larger or smaller within the world. This is not something we can do by moving the end effector. Moving the end effector closer to or further from the object, will slightly scale how large it looks in the captured image due to perspective projection, however, this movement would also affect the depth from the camera to the object, impacting the Z coordinates of the keypoints in the image. As such we do not allow the robot to consider scaling the object as part of the transformation. If the end effector is too close to or too far from the object, then this is handled through the depth information mismatching, which would compute a movement in the Z axis to fix this.\\

While this does limit the system's generalisability to novel objects which are different sizes compared to demonstration objects, it is a necessary sacrifice. Even with the same object in both images, the scale factor is rarely computed as exactly 1, due to camera perspective warping the object. This means the calculation of the scale factor is far too inaccurate to be used reliably. Adding it into the calculation for the translation causes this result to also become wildly inaccurate, decreasing the reliability of the whole system.
\\

The final modification we make is in how we compute the translation. In \refalg{alg:modified-kabsch} we compute the translation as the difference in the centroids, without the rotation and scaling applied. We already discussed why scaling is ignored. However, removing the rotation from this step is less obvious. This is a result of how our environment is set up. We decide to implement rotations about the current position. This is the case for all objects, but especially important for the end effector. When we rotate the end effector, we do not rotate about the world origin, we rotate about the end effector's current position, effectively spinning it in place. As such we can consider the translation and rotation as two independent transformations. First we spin the end effector in place to match the object's orientation. Then we move the end effector so that it is aligned relative to the object. Because the rotation happens in place, it does not have any effect on how much we need to translate by.

\begin{algorithm}
    \setlength{\baselineskip}{18pt}
    \caption{\textbf{Modified Kabsch-Umeyama algorithm}}
    \label{alg:modified-kabsch}
    \begin{algorithmic}[1]
        \Require $P = [p_1, p_2, \dots p_N], \shortspace p_i \in \real^M$
        \Statex $Q = [q_1, q_2, \dots q_N], \shortspace q_i \in \real^M$
        \Ensure $|P| = |Q| = N$
        \Procedure{Modified-Kabsch-Umeyama}{P, Q}
            \State $\overline{p} \gets \frac{1}{N}\sum^N_{i} p_i$
            \State $\overline{q} \gets \frac{1}{N}\sum^N_{i} q_i$
            \State $H \gets \frac{1}{N}\sum^N_{i} (p_i - \overline{p})^T(q_i - \overline{q})$ \Comment{Covariance Matrix}
            \State $U, \Sigma, V^T \gets \Call{SVD}{H}$ \Comment{Singular Value Decomposition}
            \State $S \gets I_{M \times M}$ \Comment{$M \times M$ Identity matrix}
            \If{$det(U) * det(V^T) == -1$}
                \State $S_{M,M} \gets -1$ \Comment{Set the bottom right corner element to $-1$}                
            \EndIf
            \State $R \gets U \cdot S \cdot V^T$
            \State $t \gets \overline{p} - \overline{q} $
    
            \State \Output $t, R$
        \EndProcedure
        
    \end{algorithmic}
\end{algorithm}

One final nuance with using this modified function is what order to pass the parameters in. We recall that the function computes the transformation which maps Q onto P. Therefore it would seem logical to pass the demonstration points as P and the live points as Q. This tells us how to move the live points so that they line up with the demonstration points. However, a subtlety is that we cannot directly move the live points by this transformation. We move the end effector and the camera in order to affect where the points will be in the next iteration. If we wish the points to move to the left of the image, then we need to move the camera to the right. Similarly to rotate the points clockwise about the Z axis, we rotate the camera anti-clockwise about the Z axis. This means after computing $t$ and $R$, we actually move the end effector by $-t$ and rotate it by $R^T$. It just so happens that $-t$ and $R^T$ are exactly the values returned from the function if you swap the inputs, computing the transformation from the demonstration points onto the live points. As such this is the order we choose to pass the parameters, to avoid needing additional calculations.

%TODO MAYBE mention experiments for how it behaves with noise applied to ideal matches.
% create random unit vector, and random magnitude of noise. This way we can ensure noise is in a tight window. So we can say this is how it perfromed with 1-5 pixels noise, 5-10. Better than 1-5 and 1-10.

\section{Final Algorithm}
With each of these modules defined, we can consider the algorithm as a whole. Following the pipeline shown in \reffig{fig:pipeline} we produce the following pseudocode:

\begin{algorithm}
    \setlength{\baselineskip}{18pt}
    \caption{\textbf{LiteBot: A lightweight One-shot Imitation Learning algorithm}}
    \label{alg:whole-system}
    \begin{algorithmic}[1]
        \Require $\mathcal{D} = \{[\tau, I^{(RGB)}, I^{(D)}, V]\}$ \Comment{set of training demonstrations}
        \Statex $\Sigma' = [I'^{(RGB)}, I'^{(D)}, V']$ \Comment{live environment context}
        \State Select demonstration
        \Repeat
            \State Extract keypoints from demonstration image and live image
            \State Match keypoints between demonstration image and live image
            \State Convert matching keypoints to world coordinates
            \State Calculate $t$, $r$ from modified Kabsch-Umeyama algorithm
            \State Move end effector by $t$ and rotate by $r$
            \State Calculate mean keypoint error
        \Until{keypoint error $>$ threshold}
        \State Calculate total amount moved $M$ and $R$
        \ForAll{keyframes $[P,O]$ in chosen demonstration}
            \State Move end effector to $P+M$ and $RO$
        \EndFor
        
    \end{algorithmic}
\end{algorithm}
%TODO make look nice, talk a bit more about why its in a loop


%Old stuff

% \section{Decomposing the reward function}
% \label{sec:reward-function}
% At this point we have a system which can transform a trajectory from the demonstration environment to a new unseen environment containing the same object. Our goal now is to extend this system to transform the trajectory into an environment containing different objects. The current system will select the demonstration whose context image matches the current scene most closely. This allows it to select the correct demonstration whose context image contains the target object in the current scene. However, this naturally extends to unseen objects since the similarity metric used will still compare the current scene to all context images and select the most similar. We can expect the similarity between the embeddings of the two images to be greatest when they contain the same objects, however if no such demonstration exists on this specific object, we can still select whichever demonstration is closest. Since we directly compare not the images themselves, but the image embeddings from the vision transformer, this translates to selecting the image with the object whose dino feature points are most similar. In essence, the object which behaves most similarly to the current scene object. Therefore we can expect the system to already choose the demonstration which is closest to working with this new object. For example if the demonstration was to pick up a can, it is likely that when faced with a bottle in the current scene that the system would select the can demonstration. This is because the objects appear similar as they are both cylindrical items, and the robot would have to interact with them in similar ways, by grasping around the cylinder. As such we can conclude that the can demonstration is the closest one to working with this new object.


% The hope would be that these two objects are similar enough that the same demonstration would already be able to successfully complete the task on this new object.

% %TODO the later stuff is no longer relevant but the above paragraph could still be good to talk about. Just slightly reword stuff to sound like this is a happy bonus rather than the original focus


% Designing the reward function to score how good an executed trajectory was will be very complicated since we need to encode so many intuitive notions into rigorous mathematics. For example, if the trajectory ends in some state $s$ then we want to ask \speech{Does this state represent some task having been completed?} This is very hard to do since the task could be anything, and we have little prior knowledge of what the robot was intending to achieve. Additionally for each state along the trajectory we would also ask questions of \speech{Is this state interesting?} \speech{Is this state unlike states we have seen before?} \speech{Was this state what we expected to happen by completing these actions, or was the outcome surprising?} We need ways to mathematically encode each of these questions from the series of state, action pairs along the robot's trajectory. We will also need to weight the relative importance between each aspect of the reward function.\\

% We start by considering how to categorise the above questions. Particularly we may notice that the question of \speech{Was a task completed?} concerns primarily the final state of the trajectory. It wants to know information external to the trajectory itself, particularly, was a meaningful task accomplished by this trajectory. The other questions however, ask about the inherent properties of the trajectory, were the states involved interesting, novel or surprising? These two different categories are called extrinsic and intrinsic rewards respectively. Our final reward function will be some combination of both types.\\
% \textbf{\textit{Extrinsic rewards}} motivate the agent to learn to complete a specific task. The specifics of the trajectory itself are unimportant, if the task has been completed then a high reward is given. The problem with extrinsic rewards is that they are by definition, dependent on the task at hand. This requires us to define a reward function for each task. This flies in direct opposition to the goal of this paper which is to produce an agent which can learn to perform new tasks which were unseen in the demonstrations. One potential solution is to use external rewards. This strategy does not have an analytically defined reward function. Instead we ask a human for input to decide the reward the robot gets for this trajectory. This could be on a scale from 0 to 1 where 1 is perfectly completing the task. This system has promise because despite requiring human input, it offloads the complex intuition required to properly score the trajectory. Specifically because the agent is attempting to solve new tasks, the human is able to infer what the robot was going for, and grade accordingly. If the robot manages to pick up an object, we can be reasonably sure this was the goal of the task, and it has therefore been completed. This high level reasoning about what the task was and subsequently has it been achieved, would be very difficult for a computer system to evaluate.\\
% \textbf{\textit{Intrinsic rewards}} in contrast, motivate the agent to produce better trajectories. For example trajectories which are different from other previously seen trajectories. These rewards are concerned only by the internal properties of the trajectories and do not care about the impact this trajectory has on the external environment.



%Even older stuff

% \chapter{Project Plan}
% \label{chap:plan}


% \section{Technical Progress}
% Technical progress so far has been limited while a focus has been put on research and writing this interim report. The main implementation achievements thus far have been on important utilities for later in the project. In particular downloading the necessary files and initialising the simulator is obviously key to the success of this project and so was completed as a priority.\\

% Following this, a utility program was created for generating demonstration trajectories. As discussed in Chapter \ref{chap:background}, this project aims to use imitation learning to teach the agent an initial collection of skills. As such, a program to streamline the collection of demonstrations for the imitation learning will be a useful asset throughout this project.\\

% This program allows for individual joint control by selecting and moving joints independently. Once the arm is positioned in the correct pose, the user can save the current pose as a key frame of the trajectory. From here they can move the arm again and save the next key frame. If a mistake is made, the user can revert the robot arm to the last saved key frame.\\
% The trajectory is stored as a list of joint poses. Each element of the trajectory is a key frame robot state. The key frame stores the joint angles of the robot's 11 joints (although not all joints are actuated so a small number of these 11 joints are permanently fixed at 0 radians).

% The program is written to interface with a video game controller, so that the user can control joint angles using an analogue input method such as the joysticks. This makes the program slightly more user friendly, as moving the joystick only slightly will move the joint very slowly and so allows for finer control.\\
% In addition to controlling joint poses, the program allows for camera rotation and zooming to better see the full robot pose, and colour codes the current joint the user is controlling in the wire-frame view, visible in Figure \ref{fig:arm-wire}.



% \section{Future Plans}
% The project plan can be broken into small short term goals which will be built upon, and longer term goals which constitute the bulk of the project. This project aims to complete the goals by the deadline shown:

% \subsection*{End of January 2024}
% Since very little time remains before the end of January, a very small goal of further improvements to the trajectory generating utility program is set. The program should be modified to allow for a second control mode. Instead of individually controlling each joint angle, it should be possible to control the 3D position of the end effector, and use inverse kinematics to calculate the joint angles from this position. The two control modes should work with each other and be able to switch between them while using the program.

% \subsection*{End of February 2024}
% By the end of February the first stage of the learning algorithm should be in place. This is implementing the one-shot imitation learning algorithm as described by  N. Di Palo, and
% E. Johns \cite{one-shot-imitation}. This first part of the algorithm will allow us to provide the agent with an initial set of skills, from which it can explore and learn more.

% \subsection*{End of March 2024}
% By the end of March this project hopes to implement the next step of the algorithm which involves the noise based exploration of the nearby state space. The agent will select a known skill trajectory and try adding noise to it to investigate what happens. The idea being that the agent selects the most similar skill from its repertoire, and so small amounts of learned noise will be sufficient for a task that nearly succeeded. For example, starting with the trajectory to pick up a water bottle, the agent may be able to learn how to pick up a can by simply augmenting the known trajectory. The action of picking up both objects are very similar, but the robot may need to grab lower on the can since it is shorter than the bottle. This slight deviation will be captured by the noise. Once an amount of noise is learned which is successful at completing the new task, the original trajectory and noise can be saved together as a new skill.\\
% This is a large part of the algorithm and March also contains end of term exams, reducing the amount of time available for the project in the final weeks of March. As such this is a soft deadline, and we anticipate the need for this implementation point to bleed into the next month's milestones.

% \subsection*{End of April 2024}
% By the end of April this project aims to be at the final implementation step. This will include the \socalled{curiosity based reward function} to encourage exploration of interesting or uncertain parts of the state space, which will hopefully lead to more unique and successful skills being learned.\\
% The other feature we plan to implement by April is the human based skill descriptor system. As discussed in Chapter \ref{chap:introduction}, this would allow the robot to demonstrate its self-taught skills where the human can provide context and a name to this skill. This can then be used in a deployed setting with a language model to give the agent orders to complete tasks based on its learned skills.

% \subsection*{End of May 2024}
% The month of May is intended to be used to run larger scale experiments and fine tune the algorithm. By this point the bulk of the implementation should be complete and this project should be looking to collect lots of results, and potentially deploy the agent to a physical robot arm. The transition from simulation to real world will undoubtedly bring challenges through the inaccuracies of real life such as joint frictions or differences in the simulated and true masses of objects and the robot arm segments. These small inaccuracies of the simulation may lead to compounding problems which affect the success rate of the agent. If this is the case then further tuning will be considered to improve the agent again.

% \subsection*{17th June 2024}
% The month of June is reserved for finishing and polishing the final report. By this deadline the project, experiments and accompanying report should all be completed.